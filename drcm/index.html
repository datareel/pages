<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="DataReel Software Development">
    <meta name="author" content="DataReel Software Development">
    <meta name="title" content="DataReel OpenSource" />
    <meta name="creator" content="DataReel Software Development" />
    <meta name="publisher" content="DataReel Software Development" />
    <meta name="contributor" content="DataReel Software Development" />
    <meta name="date.created" scheme="ISO8601" content="2016-08-20" />
    <meta name="date.reviewed" scheme="ISO8601" content="2016-08-20" />
    <meta name="language" scheme="DCTERMS.RFC1766" content="EN-US" />
    <meta name="keywords" content="Database, Linux, Applications, load
				   balancer, cluster manager,
				   Communications, Sockets, Threads, C++,
				   Programming, Windows, open source,
				   multi-threaded, cross-platform,
				   commercial, individual, academic, 
				   modular" />
    <meta name="rights" content="http://datareel.com" />
    <meta name="rating" content="General" />
    <meta name="Distribution" content="Global" />
    <meta name="Revisit-after" content="1 week">
    <meta name="robots" content="index,follow" />
    <link rel="icon" href="../images/favicon.ico">
    <title>Cluster Manager</title>
    <link href="../css/bootstrap.min.css" rel="stylesheet">
    <link href="../css/ie10-viewport-bug-workaround.css" rel="stylesheet">
    <link href="../css/starter-template.css" rel="stylesheet">
    <script src="../js/ie-emulation-modes-warning.js"></script>
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements
    and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>

    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed"
          data-toggle="collapse" data-target="#navbar"
          aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="http://datareel.com/">DataReel Open Source</a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a href="../clib/index.html">C/C++ Library</a></li>
            <li class="active"><a href="index.html">Cluster Manager</a></li>
            <li><a href="../drlb/index.html">Load Balancer</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container">

      <div>
<!-- My Content Starts Below -->

<!-- Start company logo -->

<center><img src="../images/drlogo1.jpg" /></center>
<!-- End company logo --><!-- Start Menu -->
<hr color="#000000" noshade="noshade" size="1" /><!-- Begin Title Bar --
				-- -->
<table border="0" cellpadding="0" width="100%">
  <tbody>
    <tr>
      <td bgcolor="A0CCFF"><font face="Arial,Helvetica,Sans-serif"><b>&nbsp;Contents</b></font></td>
      </tr>
    </tbody>
</table>
<br />
<!-- End Title Bar -->

<p>
<a href="#Overview">Overview</a><br />
<a href="#DocumentedUsage">Documented Usage</a><br />
<a href="#Prerequisites">Prerequisites</a><br />
<a href="#BuildingandInstallingFromSourceCode">Building and Installing From Source Code</a><br />
<a href="#DRCMAuthTokens">DRCM Auth Tokens</a><br />
<a href="#BuildingCMConfigurationFiles">Building CM Configuration Files</a><br />
<a href="#TestingCMConfigurationFiles">Testing CM Configuration Files</a><br />
<a href="#CMNodeHostnameandIPSettings">CM Node Hostname and IP Settings</a><br />
<a href="#CMGlobalConfigurationSettings">CM Global Configuration Settings</a><br />
<a href="#SettingUpGlobalClusterResources">Setting Up Global Cluster Resources</a><br />
<a href="#LinkingGlobalClusterResourcestoNodes">Linking Global Cluster Resources to Nodes</a><br />
<a href="#FloatingIPAddressInterfaceConfiguration">Floating IP Address Interface Configuration</a><br />
<a href="#RunningtheDRCMServer">Running the DRCM Server</a><br />
<a href="#DRCMResourceScripts">DRCM Resource Scripts </a><br />
<a href="#RunningApplications">Running Applications:</a><br />
<a href="#ClusterMonitorandReports">Cluster Monitor and Reports</a><br />
<a href="#2wayfilereplication">2-way file replication</a><br />
<a href="#Settingupclusteralerts">Setting up cluster alerts</a><br />
<a href="heartbeat_to_drcm.txt">Transition from heartbeat to DRCM</a><br />

<a href="#RemainingWorkonThisProject">Remaining Work on This Project</a><br />
<a href="#SupportandBugTracking">Support and Bug Tracking</a><br />

<!-- Begin Title Bar -->
<table border="0" cellpadding="0" width="100%">
  <tbody>
    <tr>
      <td bgcolor="A0CCFF"><font face="Arial,Helvetica,Sans-serif"><b>&nbsp;<a name="Overview">Overview</a></b></font></td>
    </tr>
  </tbody>
</table>
<!-- End Title Bar -->
<br />
<p>
The DataReel cluster manager (DRCM) software is used to build high
availability Linux server clusters. High availability clusters manage
system resources with automatic fail over in event of one or more
server failures. 
<br /><br />
The DRCM project was designed to support multiple
cluster nodes in a non-hierarchical arrangement. Cluster resources are
distributed between the nodes with backup rolls assigned. The DRCM
project requirements were based on the need to replace existing
cluster software with multiple dependency issues and cluster software
that did not scale well more than 2 cluster nodes. 
<br /><br />
A  Linux server
cluster is typically 2 servers with equal capacity, where one server
provides primary services and the other server provides backup
services. In the event of a primary server failure the secondary
server automatically assumes all cluster resources. In data centers
utilizing virtualization technology, services are distributed between
many virtual machines (VMs). In a VM environment clustering multiple
VM nodes running on multiple bare metal servers is a very effective
way to manage services in the event of a bare metal server
failure. 
<br /><br />
The DRCM multiple node concept can scale between nodes in one
or more data centers in the same or different geographical locations.
</p>
<p class="pull-left"><a href="#">Back to top</a></p>

<!-- Begin Title Bar -->
<table border="0" cellpadding="0" width="100%">
  <tbody>
    <tr>
      <td bgcolor="A0CCFF"><font face="Arial,Helvetica,Sans-serif"><b>&nbsp;<a name="DocumentedUsage">Documented Usage</a></b></font></td>
    </tr>
  </tbody>
</table>
<!-- End Title Bar -->
<br />
<p>
<ul>
<li> All commands prefixed with the $ prompt indicate the command is
  executed from a user account or from a service account. </li>

<li> All commands prefixed with the # prompt indicate the command is
  execute as root. </li>

<li> In a configuration file example a ... symbol indicates omitted content
  for brevity.</li>   
</ul>
</p>
<p class="pull-left"><a href="#">Back to top</a></p>

<!-- Begin Title Bar -->
<table border="0" cellpadding="0" width="100%">
  <tbody>
    <tr>
      <td bgcolor="A0CCFF"><font face="Arial,Helvetica,Sans-serif"><b>&nbsp;<a name="Prerequisites">Prerequisites</a></b></font></td>
    </tr>
  </tbody>
</table>
<!-- End Title Bar -->
<br />
<p>
DRCM resources consist of crontabs, floating IP addresses, services,
applications, and file systems. When you plan your cluster setup,
consider the following: 

<ul>
<li>Number of nodes per cluster</li>
<li>Assigning crons based on CPU, memory, disk space and bandwidth requirements</li>
<li>Floating IP address requirements, where a service is accessed via one IP address</li> 
<li>System services: http, mysql, postgresql, etc. Which node(s) needs to run services</li>
<li>File systems: NFS, cifs, etc. Which node(s) require file system mounts</li>
<li>Applications: Programs not part of the OS with user programs and scripting</li>
<li>SSH keys: keyed authentication for file synchronization and automation</li>
</ul>
</p>
<p class="pull-left"><a href="#">Back to top</a></p>

<!-- Begin Title Bar -->
<table border="0" cellpadding="0" width="100%">
  <tbody>
    <tr>
      <td bgcolor="A0CCFF"><font face="Arial,Helvetica,Sans-serif"><b>&nbsp;<a name="BuildingandInstallingFromSourceCode">Building and Installing From Source Code</a></b></font></td>
    </tr>
  </tbody>
</table>
<!-- End Title Bar -->
<br />
<p>
Developers and system admins, follow the instructions below to build
and install the DRCM server from the source code distro:

<pre>
$ ssh username@cmnode1
$ mkdir -pv ~/git
$ cd ~/git
$ git clone https://github.com/datareel/datareel.git
$ cd ~/git/datareel/cluster_manager/drcm_server
$ make
$ sudo su root -c &#39;make install_root&#39;
</pre>
The default configuration directory is: /etc/drcm
<br /><br />
NOTE: All nodes in the same cluster must have identical &quot;/etc/drcm&quot;
directories. It&#39;s recommended to design your initial configuration on a
node you designate as a cluster entry point. Then push all &quot;/etc/drcm&quot;
updates to all the nodes in the cluster.  
</p>
<p class="pull-left"><a href="#">Back to top</a></p>

<!-- Begin Title Bar -->
<table border="0" cellpadding="0" width="100%">
  <tbody>
    <tr>
      <td bgcolor="A0CCFF"><font face="Arial,Helvetica,Sans-serif"><b>&nbsp;<a name="DRCMAuthTokens">DRCM Auth Tokens</a></b></font></td>
    </tr>
  </tbody>
</table>
<!-- End Title Bar -->
<br />
<p>
All nodes in a cluster must have the same auth tokens. To generate
auth tokens for the first time or to replace existing tokens: 
<pre>
# mkdir -pv /etc/drcm/.auth
# chmod 700 /etc/drcm/.auth
# dd if=/dev/urandom bs=1024 count=2 &gt; /etc/drcm/.auth/authkey
# sha1sum /etc/drcm/.auth/authkey | cut -b1-40 &gt; /etc/drcm/.auth/authkey.sha1
# chmod 600 /etc/drcm/.auth/authkey /etc/drcm/.auth/authkey.sha1
</pre>
</p>
<p class="pull-left"><a href="#">Back to top</a></p>

<!-- Begin Title Bar -->
<table border="0" cellpadding="0" width="100%">
  <tbody>
    <tr>
      <td bgcolor="A0CCFF"><font face="Arial,Helvetica,Sans-serif"><b>&nbsp;<a name="BuildingCMConfigurationFiles">Building CM Configuration Files</a></b></font></td>
    </tr>
  </tbody>
</table>
<!-- End Title Bar -->
<br />
<p>
The default CM configuration is: /etc/drcm/cm.cfg<br />
To build a CM configuration template run the following command: 
<pre>
# /usr/sbin/drcm_server --check-config
</pre>
To build a CM configuration template in another location:
<pre>
# /usr/sbin/drcm_server --config-file=/tmp/test1.cfg --check-config
</pre>
</p>
<p class="pull-left"><a href="#">Back to top</a></p>

<!-- Begin Title Bar -->
<table border="0" cellpadding="0" width="100%">
  <tbody>
    <tr>
      <td bgcolor="A0CCFF"><font face="Arial,Helvetica,Sans-serif"><b>&nbsp;<a name="TestingCMConfigurationFiles">Testing CM Configuration Files</a></b></font></td>
    </tr>
  </tbody>
</table>
<!-- End Title Bar -->
<br />
<p>
After editing the CM configuration file you must check your CM
configuration files for errors before starting the DRCM server: 
<pre>
# /usr/sbin/drcm_server --check-config | grep -i -E &#39;error|warn|fail|info&#39;
</pre>
If you have any errors or warnings, review and edit your configuration file:
<pre>
# /usr/sbin/drcm_server --check-config 
</pre>
</p>
<p class="pull-left"><a href="#">Back to top</a></p>

<!-- Begin Title Bar -->
<table border="0" cellpadding="0" width="100%">
  <tbody>
    <tr>
      <td bgcolor="A0CCFF"><font face="Arial,Helvetica,Sans-serif"><b>&nbsp;<a name="CMNodeHostnameandIPSettings">CM Node Hostname and IP Settings</a></b></font></td>
    </tr>
  </tbody>
</table>
<!-- End Title Bar -->
<br />
<p>
After generating the cluster configuration file you must edit the file
and set up your global cluster configuration and node
configuration. For nodes you include in a cluster you must have the 
following information for each node: 
<br /><br />
Fully qualified hostname matching: uname -n<br />
IP address used for cluster keep alive and CM message handling. 
<pre>
# vi /etc/drcm/cm.cfg

[CM_NODE]
nodename = cmnode1  # This can be any name you will recognize this node as
hostname = cmnode1.example.com # uname -n
keep_alive_ip = 192.168.122.183   
...

[CM_NODE]
nodename = cmnode2
hostname = cmnode2.example.com
keep_alive_ip = 192.168.122.184
...
</pre>
<p>
<p class="pull-left"><a href="#">Back to top</a></p>

<!-- Begin Title Bar -->
<table border="0" cellpadding="0" width="100%">
  <tbody>
    <tr>
      <td bgcolor="A0CCFF"><font face="Arial,Helvetica,Sans-serif"><b>&nbsp;<a name="CMGlobalConfigurationSettings">CM Global Configuration Settings</a></b></font></td>
    </tr>
  </tbody>
</table>
<!-- End Title Bar -->
<br />
<p>
The CM_SERVER configuration setting sets values used by the DRCM
service and all cluster nodes. Before starting the DRCM service, set
the CM port number and host-based firewall rules: 
<pre>
# Global cluser configuration
[CM_SERVER]
udpport = 53897
tcpport = 53897
...
</pre>
Each cluster node must have host-based firewall rules allowing the
DRCM server access to the TCP and UDP ports set in CM global
configuration:
<pre>
# iptables -N CLUSTERMANAGER
# iptables -I INPUT 3 -j CLUSTERMANAGER
# iptables -A CLUSTERMANAGER -p UDP --dport 53897 -s 192.168.122.183/32 -j ACCEPT 
# iptables -A CLUSTERMANAGER -p UDP --dport 53897 -s 192.168.122.184/32 -j ACCEPT 
# iptables -A CLUSTERMANAGER -p TCP --dport 53897 -s 192.168.122.183/32 -j ACCEPT 
# iptables -A CLUSTERMANAGER -p TCP --dport 53897 -s 192.168.122.184/32 -j ACCEPT 
</pre>
</p>
<p class="pull-left"><a href="#">Back to top</a></p>

<!-- Begin Title Bar -->
<table border="0" cellpadding="0" width="100%">
  <tbody>
    <tr>
      <td bgcolor="A0CCFF"><font face="Arial,Helvetica,Sans-serif"><b>&nbsp;<a name="SettingUpGlobalClusterResources">Setting Up Global Cluster Resources</a></b></font></td>
    </tr>
  </tbody>
</table>
<!-- End Title Bar -->
<br />
<p>
All nodes in a cluster has the ability to run a global cluster
resource as a primary function or as a backup function. DRCM has 5
global resource categories:
<br /><br />
CM_CRONTABS: crons to run on a node with fail over to secondary nodes
CM_IPADDRS: floating IP addresses to run on a node with fail over to secondary nodes
CM_SERVICES: System services to run on all nodes or single nodes
CM_APPLICATIONS: Applications to run on all nodes or single nodes
CM_FILESYSTEMS: File system to mount on all nodes or single nodes
<br /><br />
Under each resource section you must provide a CSV list for each
entry. All resources subsections start with a nickname. A nickname can
be any unique name you will recognize the resource as. Nicknames are
used by the cluster nodes to identify the global resource. 
<pre>
[CM_CRONTABS]
# CSV format: nickname, template_file, install_location, resource_script
system, /etc/drcm/crontabs/system_crons, /etc/cron.d, /etc/drcm/resources/crontab.sh
user, /etc/drcm/crontabs/user_crons, /etc/cron.d, /etc/drcm/resources/crontab.sh
apps1, /etc/drcm/crontabs/apps_package1, /etc/cron.d, /etc/drcm/resources/crontab.sh
apps2, /etc/drcm/crontabs/apps_package2, /etc/cron.d, /etc/drcm/resources/crontab.sh

[CM_IPADDRS]
# CSV format: nickname, Floating IP, netmask, Ethernet interface, ip takeover script
cmnodef, 192.168.122.180, 255.255.255.0, eth0:1, /etc/drcm/resources/ipv4addr.sh

[CM_SERVICES]
# CSV format: nickname, service_name, resource_script
web, httpd, /etc/drcm/resources/service.sh

[CM_APPLICATIONS]
# CSV format: nickname, user:group, start_program, stop_program
# CSV format: nickname, user:group, start_program, stop_program, ensure_script
ldm, ldm:users, ~/util/ldm_ensure.sh, ldmadmin stop, ~/util/ldm_ensure.sh

[CM_FILESYSTEMS]
# CSV format: nickname, source, target, resource_script
data, 192.168.122.1:/data, /data, /etc/drcm/resources/nfs.sh
archive, 192.168.122.1:/archive, /archive, /etc/drcm/resources/nfs.sh
webshare, //192.168.122.225/users/web, /home/users/web, /etc/drcm/resources/cifs.sh
</pre>
</p>
<p class="pull-left"><a href="#">Back to top</a></p>

<!-- Begin Title Bar -->
<table border="0" cellpadding="0" width="100%">
  <tbody>
    <tr>
      <td bgcolor="A0CCFF"><font face="Arial,Helvetica,Sans-serif"><b>&nbsp;<a name="LinkingGlobalClusterResourcestoNodes">Linking Global Cluster Resources to Nodes</a></b></font></td>
    </tr>
  </tbody>
</table>
<!-- End Title Bar -->
<br />
<p>
The global cluster resource for a node is set in each CM_NODE
section. Below is a 2 node cluster example of multiple crontabs with
primary and secondary rolls. A single floating IP address for cluster
users to access this cluster. In this example system services, file
systems, and applications will be ran on both nodes.   
<pre>
[CM_NODE]
nodename = cmnode1
...
node_crontabs = user,system,apps1
node_backup_crontabs = cmnode2:apps2
node_floating_ip_addrs = cmnodef
node_services = web
node_filesystems = data,arhive,webshare
node_applications = ldm

[CM_NODE]
nodename = cmnode2
...
node_crontabs = apps2
node_backup_crontabs = cmnode1:user,cmnode1:system
node_backup_floating_ip_addrs = cmnode1:cmnodef
node_services = web
node_filesystems = data,arhive,webshare
node_applications = ldm
</pre>
</p>
<p class="pull-left"><a href="#">Back to top</a></p>

<!-- Begin Title Bar -->
<table border="0" cellpadding="0" width="100%">
  <tbody>
    <tr>
      <td bgcolor="A0CCFF"><font face="Arial,Helvetica,Sans-serif"><b>&nbsp;<a name="FloatingIPAddressInterfaceConfiguration">Floating IP Address Interface Configuration</a></b></font></td>
    </tr>
  </tbody>
</table>
<!-- End Title Bar -->
<br />
<p>
If your cluster has one or more floating IP addresses you must
configure a sub-interface for each floating IP. A floating IP address
is set in the global resource configuration  CM_IPADDRS section, for
example:
<pre>
[CM_IPADDRS]
cmnodef, 192.168.122.180, 255.255.255.0, eth0:1, /etc/drcm/resources/ipv4addr.sh
</pre>
In the example above, we need to setup a sub-interface for eth0:
<pre>
# cd /etc/sysconfig/network-scripts
# vi ifcfg-eth0:1

DEVICE=eth0:1
BOOTPROTO=none
ONBOOT=yes
</pre>
Save changes and exit VI.
<br /><br />
IPADDR and PREFIX or NETMASK will be set by DRCM server when the node
resource is activated.
<pre>
# ifup eth0:1
</pre>
</p>
<p class="pull-left"><a href="#">Back to top</a></p>

<!-- Begin Title Bar -->
<table border="0" cellpadding="0" width="100%">
  <tbody>
    <tr>
      <td bgcolor="A0CCFF"><font face="Arial,Helvetica,Sans-serif"><b>&nbsp;<a name="RunningtheDRCMServer">Running the DRCM Server</a></b></font></td>
    </tr>
  </tbody>
</table>
<!-- End Title Bar -->
<br />
<p>
After setting up your cluster configuration and mirroring the
&quot;/etc/drcm&quot; directory to all nodes in the cluster, start the DRCM
server on each node. 
<br /><br />
RHEL 7/CENTOS 7:
<pre>
# systemctl start drcm_server
# systemctl status drcm_server
</pre>
RHEL 6/CENTOS 6:
<pre>
# service drsm_server start
# service drcm_server status
</pre>
View the cluster status monitor:
<pre>
# drcm_server --client --command=cm_stat
</pre>
View the cluster logs:
<pre>
# tail -f /var/drcm/drcm.log
</pre>
To test resource fail over:
<pre>
# systemctl stop drcm_server
# drcm_server --client --command=cm_stat
</pre>
To test resource fail back:
<pre>
# systemctl start drcm_server
# drcm_server --client --command=cm_stat
</pre>
After testing make the DRCM service persistent:
<br /><br />
RHEL 7/CENTOS 7:
<pre>
# systemctl enable drcm_server
</pre>
RHEL 6/CENTOS 6:
<pre>
# chkconfig drsm_server on
</pre>
</p>
<p class="pull-left"><a href="#">Back to top</a></p>

<!-- Begin Title Bar -->
<table border="0" cellpadding="0" width="100%">
  <tbody>
    <tr>
      <td bgcolor="A0CCFF"><font face="Arial,Helvetica,Sans-serif"><b>&nbsp;<a name="DRCMResourceScripts">DRCM Resource Scripts</a></b></font></td>
    </tr>
  </tbody>
</table>
<!-- End Title Bar -->
<br />
<p>
The default DRCM resource script location is: /etc/drcm/resources
<br /><br />
The default installation include system resource script to manage
crontabs, floating IP addresses, system services, NFS and CIFS files
systems. The NFS and CIFS file system resource script will check for
stale mounts and automatically remount if a mount is stale.  
<br /><br />
Which resource scripts used is set in the global resource
configuration. Cluster administrators can use custom resource scripts,
provided custom scripts accept the same input arguments as the
resource scripts included in the distribution. 
</p>
<p class="pull-left"><a href="#">Back to top</a></p>

<!-- Begin Title Bar -->
<table border="0" cellpadding="0" width="100%">
  <tbody>
    <tr>
      <td bgcolor="A0CCFF"><font face="Arial,Helvetica,Sans-serif"><b>&nbsp;<a name="Running_Applications">Running Applications</a></b></font></td>
    </tr>
  </tbody>
</table>
<!-- End Title Bar -->
<br />
<p>
Applications refer to programs that are not part of the operating
system (OS) or integrated into the OS framework. Managing user
applications in a cluster can be difficult. The DRCM server does not
manage user applications from resource scripts. This allows
flexibility to run applications from a user or service account and use
programs and scripting unique to each application. Applications are
defined in the global CM_APPLICATIONS section: 
<pre>    
[CM_APPLICATIONS]
</pre>
There are 2 formats to define a user application:
<pre>
# CSV format: nickname, user:group, start_program, stop_program
# CSV format: nickname, user:group, start_program, stop_program, ensure_script
</pre>
The user and group must be set to the username that will run the
application and the group associated with application.  
<br /><br />
The start_program is the program used to start the application, with
or without input arguments. This can be multiple entries separated by
semicolons 
<br /><br />
The stop_program is the program used to start the application, with
or without input arguments. This can be multiple entries separated by
semicolons 
<br /><br /> 
The ensure_script is a script or program that checks to ensure the
application is running. If an ensure script is provided the DRCM
server will use the &quot;ensure_script&quot; to watch the application. 
<br /><br />
NOTE: The user environment will be sourced so you do not have to use
absolute paths to programs if the program path is defined in the user
environment.   
<br /><br />
Some examples:
<pre>
ldm, ldm:users, ldmadmin start, ldmadmin stop

ldm, ldm:users, ldmadmin clean; ldmadmin mkqueue; ldmadmin start, ldmadmin stop; ldmadmin delqueue

ldm, ldm:users, ldmadmin start, ldmadmin stop, if [ -f ~/util/ldm_ensure.sh ]; then ~/util/ldm_ensure.sh; fi
</pre>
</p>
<p class="pull-left"><a href="#">Back to top</a></p>

<!-- Begin Title Bar -->
<table border="0" cellpadding="0" width="100%">
  <tbody>
    <tr>
      <td bgcolor="A0CCFF"><font face="Arial,Helvetica,Sans-serif"><b>&nbsp;<a name="ClusterMonitorandReports">Cluster Monitor and Reports</a></b></font></td>
    </tr>
  </tbody>
</table>
<!-- End Title Bar -->
<br />
<p>
DRCM provides a formatted reporting tool and raw stats you can use to
build custom reports. To view the detailed cluster report, from any
cluster node:
<pre>
/usr/sbin/drcm_server --client --command=cm_stat
</pre>
To create custom reports use the rstats command from any cluster node, for example:
<pre>
for CMNODE in cmnode1 cmnode2 cmnode3 cmnode4
&gt; do
&gt; echo $CMNODE
&gt; /usr/sbin/drcm_server --client --command=rstats | sed -n &quot;/$CMNODE/,/$CMNODE/p&quot; | grep -v $CMNODE
&gt; echo &quot;&quot;
&gt; done
</pre>
</p>
<p class="pull-left"><a href="#">Back to top</a></p>

<!-- Begin Title Bar -->
<table border="0" cellpadding="0" width="100%">
  <tbody>
    <tr>
      <td bgcolor="A0CCFF"><font face="Arial,Helvetica,Sans-serif"><b>&nbsp;<a name="2wayfilereplication">2-way file replication</a></b></font></td>
    </tr>
  </tbody>
</table>
<!-- End Title Bar -->
<br />
<p>
The DRCM distribution includes several utilities to synchronize file
systems between a primary and secondary cluster node. This is
typically used when a primary node is acting as a NFS server for other
servers and/or workstations. Or for primary nodes replicating database
directories to a cold spare.  
<br /><br />
NOTE: File replication requires public SSH key authentication for the
backup user account that will run the replication scripts from the
user&#39;s crontab. NOTE: If you are using &quot;root&quot; to run system backups
you can limit access to key SSH authentication to run specific
commands. Typically the back user is a special user account used just
for running file system backups. 
<br /><br />
NOTE: Your backup user must have read/write access to the following
directory: 
<pre>
/etc/drcm/my_cluster_conf
</pre>
Following the DRCM installation, the file system utilities are located
in the following directory: 
<pre>
/etc/drcm/file_system_sync
</pre>
To configure a designated primary node and a designated backup node
you must set you node configuration in the following file: 
<pre>
/etc/drcm/my_cluster_conf/my_cluster_info.sh
</pre>
In the &quot;my_cluster_info.sh&quot; file set the following variables:
<pre>
# Set a simple cluster name, any name you will recognize
# this cluster as, no spaces.
export CLUSTER_NAME=&quot;NFS-CLUSTER1&quot;

# Set IP or hostname of online storage server for shared backups
# This is only need if you plan to backup the NFS or database 
# directories to a backup storage severs.
export BACKUP_STORAGE_SERVER=&quot;192.168.122.1&quot;

# Set the destination directory on the storage server
export BACKUP_STORAGE_LOCATION=&quot;/backups&quot;

# Set the user name that runs you backups.
# NOTE: This user name must have keyed SSH authentication access
# NOTE: to the cluster nodes and backup storage server
export BACKUP_USER=&quot;root&quot;

#
# Set the following for 2-way file system replication
#
# IP address and Ethernet Interface of the cluster node with primary
# file systems. File systems on this node will be replicated to the
# backup file system in the event of a fail over to the backup node.
export PRIMARY_FILESYSTEMS_IP=&quot;192.168.122.111&quot;
export PRIMARY_FILESYSTEMS_ETH=&quot;eth0&quot;
#
# IP address and Ethernet Interface of the backup for this cluster node,
# where the primary file systems are mirrored too. When the primary
# node is down this node will host the file systems.
export BACKUP_FILESYSTEMS_IP=&quot;192.168.122.112&quot;
export BACKUP_FILESYSTEMS_ETH=&quot;eth0&quot;

# Allow RSYNC on a diffent IP/Interface if high bandwidth required to sync file systems
# These settings are used to move high bandwidth consumption to another Ethernet interface 
# and/or network.                                                         
## export PRIMARY_FILESYSTEMS_RSYNC_IP=&quot;&quot;
## export BACKUP_FILESYSTEMS_RSYNC_IP=&quot;&quot;
</pre>
Once you setup your configuration, edit the following file to run a
test between the primary and backup node. As the backup user on the
primary node: 
<pre>
$ vi /etc/drcm/my_cluster_conf/testfs_sync_list.sh

RsyncMirror /testfs
</pre>
NOTE: Mirroring replicated an exact copy of the /testfs file
system. This means files will be copied and deleted as new files are
added or removed. If you want to keep copies of files removed on the
backup system use the following line 
<pre>
RsyncCopy /testfs
</pre>
NOTE: Prior to testing you must create the /testfs directory on both
cluster nodes and make sure your backup user has read/write access to
the /testfs directory.
<br /><br />
Make sure to copy all changes in the /etc/drcm directory to the backup
node before you do any testing. To test, as the backup user on the
primary node, execute the following script: 
<pre>
$ /etc/drcm/file_system_sync/sync_testfs.sh
</pre>
This will replicate the contents of the /testfs directory to the
backup node. As the backup user on the backup node, execute the same
script: 
<pre>
$ /etc/drcm/file_system_sync/sync_testfs.sh
</pre>
You should see the following output on the backup node:
<br /><br />
Checking to see if failback sync flag exists
We have no sync flag for testfs files
No file sync to primary cluster node will be performed
<br /><br />
If the primary node is down the backup node will update the /testfs
directory until the primary node is back online. Once the primary node
is back online, all files on the backup will be replicated back to the
primary node. Once the back node has finished replication, the primary
node replicates to the back node. To test a primary to backup fail
over and fail back, execute the following script on the primary node,
as root or an admin account with sudo rights: 
<pre>
# /etc/drcm/utils/manual_fs_failover.sh
</pre>
To fail back, start the drcm_server on the primary node. 
<br /><br />
After verifying 2-way file replication is work between the primary and
backup node, add the following line to the backup users crontab. On
the primary node as the backup user: 
<pre>
$ crontab -e

*/5 * * * *  /etc/drcm/file_system_sync/sync_testfs.sh &gt; /dev/null 2&gt;&amp;1
</pre>
On the backup node:
<pre>
$ crontab -e

* * * * * /etc/drcm/file_system_sync/sync_testfs.sh &gt; /dev/null 2&gt;&amp;1
</pre>
This will replicate /testfs to the backup node every 5 minutes. The
backup node will watch for primary node outages every minute. In the
event of a fail back, the backup node will replicate /testfs back to
primary within one minte. 
<br /><br />
The DRCM distribution includes pre-built file replication scripts you
can add to the back user’s crontab: 
<br /><br />
On primary node:
<pre>
*/5 * * * *  /etc/drcm/file_system_sync/sync_mysql.sh &gt; /dev/null 2&gt;&amp;1
*/5 * * * *  /etc/drcm/file_system_sync/sync_pgsql.sh &gt; /dev/null 2&gt;&amp;1
*/6 * * * *  /etc/drcm/file_system_sync/sync_cifs.sh &gt; /dev/null 2&gt;&amp;1
*/7 * * * *  /etc/drcm/file_system_sync/sync_nfs.sh &gt; /dev/null 2&gt;&amp;1
*/8 * * * *  /etc/drcm/file_system_sync/sync_www.sh &gt; /dev/null 2&gt;&amp;1
*/11 * * * * /etc/drcm/file_system_sync/sync_other.sh &gt; /dev/null 2&gt;&amp;1
</pre>
On backup node:
<pre>
* * * * * /etc/drcm/file_system_sync/sync_mysql.sh &gt; /dev/null 2&gt;&amp;1
* * * * * /etc/drcm/file_system_sync/sync_pgsql.sh &gt; /dev/null 2&gt;&amp;1
* * * * * /etc/drcm/file_system_sync/sync_cifs.sh &gt; /dev/null 2&gt;&amp;1
* * * * * /etc/drcm/file_system_sync/sync_nfs.sh &gt; /dev/null 2&gt;&amp;1
* * * * * /etc/drcm/file_system_sync/sync_www.sh &gt; /dev/null 2&gt;&amp;1
* * * * * /etc/drcm/file_system_sync/sync_other.sh &gt; /dev/null 2&gt;&amp;1
</pre>
All the configuration files will be autogened in the following:
<pre>
/etc/drcm/my_cluster_conf
</pre>
NOTE: Your backup user must have write access to the &quot;my_cluster_conf&quot;
directory. 
<br /><br />
Edit the &quot;/etc/drcm/my_cluster_conf/*sync_list.sh&quot; config files to
setup the specific directories to be replicated. 
</p>
<p class="pull-left"><a href="#">Back to top</a></p>

<!-- Begin Title Bar -->
<table border="0" cellpadding="0" width="100%">
  <tbody>
    <tr>
      <td bgcolor="A0CCFF"><font face="Arial,Helvetica,Sans-serif"><b>&nbsp;<a name="Settingupclusteralerts">Setting up cluster alerts</a></b></font></td>
    </tr>
  </tbody>
</table>
<!-- End Title Bar -->
<br />
<p>
To setup an hourly cluster monitor with email and/or text message
alerts, add the following line to root’s crontab: 
<pre>
00 * * * * /etc/drcm/utils/watch_cluster.sh &gt; /dev/null 2&gt;&amp;1
</pre>
The watch_cluster.sh script will automatically generate the following
an alert list configuration file for you: 
<pre>
/etc/drcm/my_cluster_conf/cm_alert_list.sh
</pre>
NOTE: If you run the watch_cluster.sh script as another user, the user
must have read access to the /etc/drcm/.auth directory and auth keys.
<br /><br />
In your &quot;cm_alert_list.sh&quot; file set up the email accounts and/or SMS
accounts:
<pre>
# vi /etc/drcm/my_cluster_conf/cm_alert_list.sh

# Set your comma delimited alert lists here
export EMAILlist=&quot;root,name@example.com&quot;
export TEXTlist=&quot;555-555-5555@mobile_domain.com&quot;
</pre>
NOTE: Each mobile carrier has a domain for sending an email as a text
message, for example to send SMS messages to a Verizon number: 
<pre>
555-555-5555@vtext.com
</pre>
By default the DRCM alerts are limited to send messages every 4 hours,
once an the first message is sent. This is done to avoid a flood email
and/or test messages during periods of server outages or server
maintenance.  
<br /><br />
If your clusters nodes do not have postfix mail configured, you need
to determine which Linux server in your data center is setup to relay
messages. To setup postfix mail to connect to relay host: 
<pre>
# vi /etc/postfix/main.cf

relayhost = 192.168.122.1
</pre>
Restart the postfix service and watch the mail log:
<pre>
# tail -f /var/log/maillog
</pre>
To test the alerting function:
<pre>
$ vi ~/test_alert.sh 

#!/bin/bash

export ALERTTIMESPANHOURS=&quot;0&quot;
SUBJECT=&quot;[!ACTION!] Cluster Alert Test Message&quot;
BODY=&quot;This is an cluster manager alert test message&quot;
source /etc/drcm/utils/send_alert.sh
send_alert &quot;${SUBJECT}&quot; &quot;${BODY}&quot;
</pre>
Save file and exit VI.
<pre>
$ chmod 755 ~/test_alert.sh
$ ~/test_alert.sh
</pre>
NOTE: The DRCM alert functions can be use by other system admin
functions by writing scripts similar to the above example. 
<br /><br />
If you do not have a Linux system setup up as a relay host, you can
setup postfix on one cluster node as a smart host and allow other
nodes to relay messages. For more information on setting up postfix
mail as a smart host, search online for: &quot;postfix smarthost setup&quot;
</p>
<p class="pull-left"><a href="#">Back to top</a></p>

<!-- Begin Title Bar -->
<table border="0" cellpadding="0" width="100%">
  <tbody>
    <tr>
      <td bgcolor="A0CCFF"><font face="Arial,Helvetica,Sans-serif"><b>&nbsp;<a name="RemainingWorkonThisProject">Remaining Work on This Project</a></b></font></td>
    </tr>
  </tbody>
</table>
<!-- End Title Bar -->
<br />
<p>
The first DRCM release was successfully tested on VM and bare metal
clusters. Implementation will continue on many data systems. All
source code changes and bug fixes will be release in this open source
distribution.    
<br /><br />
Remaining work to be added to this open source project:
<ul>
<li>RPM install for CENTOS/RHEL 6.X and 7.X</li>
<li>Add TCP cluster protocol for WAN based clusters</li>
</ul>
</p>
<p class="pull-left"><a href="#">Back to top</a></p>

<!-- Begin Title Bar -->
<table border="0" cellpadding="0" width="100%">
  <tbody>
    <tr>
      <td bgcolor="A0CCFF"><font face="Arial,Helvetica,Sans-serif"><b>&nbsp;<a name="SupportandBugTracking">Support and Bug Tracking</a></b></font></td>
    </tr>
  </tbody>
</table>
<!-- End Title Bar -->
<br />
<p>
<a href="https://github.com/datareel/datareel/issues">https://github.com/datareel/datareel/issues</a>
</p>
<p class="pull-left"><a href="#">Back to top</a></p>
<br />
<hr color="#000000" noshade="noshade" size="1" /><!-- Start Copyright Information -->
<p><font color="000000">Copyright &copy; 2001-2016&nbsp;DataReel OpenSource</font></p>

	
<!-- My Content Ends Above -->
      </div>
    </div><!-- /.container -->

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster
    -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script>window.jQuery ||
    document.write('<script src="../js/vendor/jquery.min.js"><\/script>')</script>
    <script src="../js/bootstrap.min.js"></script>
    <script src="../js/ie10-viewport-bug-workaround.js"></script>
  </body>
</html>
